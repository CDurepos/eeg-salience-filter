{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8871c87-3e58-4abb-9dc8-1041537ffc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65e332-062b-44c0-9dc8-3e20b1717d15",
   "metadata": {},
   "source": [
    "## Building the 1D-CNN with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43f905e2-1ad6-412b-83ed-fc258a34e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # First conv layer (uses passed in in_channels, NOT hard-coded)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Downsample (skip connection) if needed\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Apply skip path if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2df5c6dc-44e8-4e92-ab6d-4e5966efbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution \"stem\"\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual stages\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling over time dimension\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # output: (batch, channels, 1)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc = nn.Linear(512, n_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        # First block may change channels/stride\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride=stride))\n",
    "        # Remaining blocks keep same channels/stride=1\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, n_channels, n_times)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)  # shape: (batch, 64, T1)\n",
    "        x = self.layer2(x)  # shape: (batch, 128, T2)\n",
    "        x = self.layer3(x)  # shape: (batch, 256, T3)\n",
    "        x = self.layer4(x)  # shape: (batch, 512, T4)\n",
    "        \n",
    "        # Global average pooling: average over time dimension\n",
    "        x = self.global_pool(x)  # (batch, 512, 1)\n",
    "        x = x.squeeze(-1)        # (batch, 512)\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.fc(x)      # (batch, n_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ac935-d22c-4a3f-9620-faf71cb04d70",
   "metadata": {},
   "source": [
    "## Building the training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99dd283f-4d57-49e7-af87-0930a2cc895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.float()\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40692463-afd7-4c4e-92f3-0c9328b2797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and Y based on subjects \n",
    "X_raw = np.load(\"../data/X_tqwt_wpd.npy\")\n",
    "y_raw = np.load(\"../data/y_labels.npy\")\n",
    "\n",
    "subject_ids = np.load(\"../data/subject_ids.npy\", allow_pickle=True)\n",
    "unique_subs = np.unique(subject_ids)\n",
    "\n",
    "# Not required \n",
    "# print(\"Number of subjects:\", len(unique_subs))\n",
    "# print(\"Subject IDs:\", unique_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9f9f78-6f5c-4dc4-aa40-55f934ada254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subs, val_subs = train_test_split(\n",
    "    unique_subs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5690d141-40d0-4331-8883-4776b3580e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically ensures that there is no leakage \n",
    "\n",
    "train_mask = np.isin(subject_ids, train_subs)\n",
    "val_mask = np.isin(subject_ids, val_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cf7a0c3-9481-456a-96f3-45127677475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/X_tqwt_wpd.npy\")\n",
    "y = np.load(\"../data/y_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec696dcc-f884-4cc8-8152-e061795ad6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (16749, 1140)\n",
      "y shape: (16749,)\n",
      "subject_ids shape: (16749,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"subject_ids shape:\", subject_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43b08355-f8a0-4f6a-a63e-ec74e6da25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "X1 = X[train_mask]\n",
    "y1 = y[train_mask]\n",
    "\n",
    "X_val_np = X[val_mask]\n",
    "y_val_np = y[val_mask]\n",
    "\n",
    "\n",
    "X_train_np = torch.tensor(X1, dtype=torch.float32)\n",
    "y_train_np = torch.tensor(y1, dtype=torch.long)\n",
    "\n",
    "X_val_np = torch.tensor(X_val_np, dtype=torch.float32)\n",
    "y_val_np = torch.tensor(y_val_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5e1e8-4c27-4146-a491-2b90e849c5df",
   "metadata": {},
   "source": [
    "### Lets save the files once... No need to do this if you already have the files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a11e9d3b-797a-44f3-a4a2-3fdeb435009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../data/X_train.npy\", X_train_np)\n",
    "# np.save(\"../data/y_train.npy\", y_train_np)\n",
    "# np.save(\"../data/X_val.npy\", X_val_np)\n",
    "# np.save(\"../data/y_val.npy\", y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbfbc534-28f3-416e-ba04-207103589d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping input to DataLoader:\n",
      "(13305, 1140)\n",
      "Raw X_val shape: (3444, 1140)\n",
      "after reshaping input to DataLoader:\n",
      "(13305, 1, 1140)\n",
      "Raw X_val shape: (3444, 1, 1140)\n"
     ]
    }
   ],
   "source": [
    "# even though they were converted to tensors before saving they need to be converted back becsuse when loading them with np it becomes an array again.\n",
    "X_train = np.load(\"../data/X_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "X_val   = np.load(\"../data/X_val.npy\")\n",
    "y_val   = np.load(\"../data/y_val.npy\")\n",
    "\n",
    "\n",
    "print(\"Before reshaping input to DataLoader:\")\n",
    "print(X_train.shape)\n",
    "print(\"Raw X_val shape:\",   X_val.shape)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), 1, -1)\n",
    "X_val   = X_val.reshape(len(X_val), 1, -1)\n",
    "\n",
    "print(\"after reshaping input to DataLoader:\")\n",
    "print(X_train.shape)\n",
    "print(\"Raw X_val shape:\",   X_val.shape)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b421ed00-fbb9-4374-a17a-2d95f25402a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataloaders \n",
    "\n",
    "train_dataset = EEGDataset(X_train, y_train)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                           \n",
    "val_dataset = EEGDataset(X_val, y_val)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51c33ac7-630b-4419-8395-d28aabd0a875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN BATCH SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL BATCH SHAPE: torch.Size([32, 1, 1140])\n",
      "X_train shape from file: torch.Size([13305, 1, 1140])\n",
      "X_val shape from file: torch.Size([3444, 1, 1140])\n",
      "First element type: <class 'torch.Tensor'>\n",
      "First element shape: torch.Size([1, 1140])\n"
     ]
    }
   ],
   "source": [
    "# Safety Check\n",
    "for xb, yb in train_loader:\n",
    "    print(\"TRAIN BATCH SHAPE:\", xb.shape)\n",
    "    break\n",
    "\n",
    "for xb, yb in val_loader:\n",
    "    print(\"VAL BATCH SHAPE:\", xb.shape)\n",
    "    break\n",
    "\n",
    "print(\"X_train shape from file:\", X_train.shape)\n",
    "print(\"X_val shape from file:\", X_val.shape)\n",
    "\n",
    "print(\"First element type:\", type(X_train[0]))\n",
    "print(\"First element shape:\", getattr(X_train[0], 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34bbd68f-ff07-4eaf-99be-f5399f1a8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialze the CNN\n",
    "\n",
    "n_channels = 1      # or however many EEG channels we have\n",
    "n_classes = 2        # ADHD vs Control\n",
    "\n",
    "model = ResNet1D(n_channels=n_channels, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33a0a39a-01c4-4250-bb56-3dc86f6612ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letting the computer know what piece of hardware to run the training \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841ae2f-ec04-4a17-aa17-8167e424bb72",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bde9f48-121a-4de8-b6ce-b4e3e3d0c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy calculations\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == y).sum().item()\n",
    "    total = y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba2ce2b0-c52d-4157-8d8e-375695b232aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # put model in \"training mode\"\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        print(\"TRAIN LOOP SHAPE:\", X.shape)\n",
    "        break\n",
    "        \n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        # 2. Compute loss\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # 3. Zero out old gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training accuracy & loss\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "#only run to check if functional. and last i ran it was functional\n",
    "#train_one_epoch(model, train_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28d89e14-74fc-4123-8d07-9fbf4abd071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid loop with no gradient Updates \n",
    "# Very similar to the training loop, except this one sets the model to eval and its accompanied by other\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Do NOT track gradients\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            print(\"VAL LOOP SHAPE:\", X.shape)\n",
    "            break\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "#only run to check if functional. and last i ran it was functional\n",
    "#validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "125ef7cb-8a43-45fb-a02f-3e43623581bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=20, lr=1e-3):\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\":  [],\n",
    "        \"val_loss\":   [],\n",
    "        \"val_acc\":    []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6f6f3-fa2b-4eb2-9cf6-48c56761a33d",
   "metadata": {},
   "source": [
    "## Training the architectiure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc8ad2d7-5e01-4200-8a54-97c181d610f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Training the resnet architeciure on Raw data \u001b[39;00m\n\u001b[1;32m      3\u001b[0m resnet_raw \u001b[38;5;241m=\u001b[39m ResNet1D( n_channels\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] , n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m history_resnet_raw \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Saving for Saliency later\u001b[39;00m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(resnet_raw\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_eeg.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr)\u001b[0m\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m:  [],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:   [],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m:    []\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 11\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m     16\u001b[0m         model, val_loader, criterion, device\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[44], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 4. Compute gradients\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 5. Update weights\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training the resnet architeciure on Raw data \n",
    "\n",
    "resnet_raw = ResNet1D( n_channels=X_train.shape[1] , n_classes=2).to(device)\n",
    "\n",
    "history_resnet_raw = train_model(\n",
    "    model=resnet_raw,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "torch.save(resnet_raw.state_dict(), \"resnet_eeg.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"resnet_eeg.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a826eeb-3d56-4f96-9316-d40a3b552279",
   "metadata": {},
   "source": [
    "## Compute saliency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93cd681b-62eb-441e-acc1-b271245ca707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(model, X_batch, y_batch, device):\n",
    "    model.eval()\n",
    "    \n",
    "    X_batch = X_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    X_batch.requires_grad = True  # Enable gradient wrt input\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(X_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient wrt input\n",
    "    saliency = X_batch.grad.detach().abs()  # absolute gradient\n",
    "\n",
    "    return saliency.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d1121-6e74-4438-a419-6499e472b3ca",
   "metadata": {},
   "source": [
    "## Global saliency vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "514d45ff-2a2f-4957-b90b-46310c9822b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_saliencies = []\n",
    "\n",
    "for Xb, yb in val_loader:\n",
    "    sal = compute_saliency(model, Xb, yb, device)\n",
    "    all_saliencies.append(sal)\n",
    "\n",
    "all_saliencies = np.concatenate(all_saliencies, axis=0)   # (N_val, 1, 1140)\n",
    "\n",
    "# Average across samples and channel\n",
    "global_saliency = all_saliencies.mean(axis=0).squeeze(0)  # (1140,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "764b5fb6-161c-41d7-87d0-703b3c5d30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "global_saliency = global_saliency / global_saliency.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e4488fd-d338-4c98-9056-f4b8fa9328a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (1140,)\n"
     ]
    }
   ],
   "source": [
    "# Ceating a binary mask \n",
    "threshold = 0.2  # keep top 80% important features or however much we wanna keep\n",
    "mask = (global_saliency > threshold).astype(np.float32) # does this not return a boolean ?? \n",
    "\n",
    "\n",
    "print(\"Mask shape:\", mask.shape)  # (1140,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a499c-436a-447d-b557-dfe9bb32fcef",
   "metadata": {},
   "source": [
    "## Handling the Masked data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a4d6479-638d-420d-af92-c790d48eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying saliency mask to the dataset, actually just uses simple multiplication\n",
    "\n",
    "# Convert tensors back to numpy so we can multiply with mask\n",
    "X_train_np = X_train.cpu().numpy().reshape(len(X_train), 1140)\n",
    "X_val_np   = X_val.cpu().numpy().reshape(len(X_val), 1140)\n",
    "\n",
    "#applying the mask\n",
    "X_train_masked = X_train_np * mask\n",
    "X_val_masked   = X_val_np * mask\n",
    "\n",
    "#reshaping back to conv1d format \n",
    "X_train_masked = X_train_masked.reshape(len(X_train_masked), 1, 1140)\n",
    "X_val_masked   = X_val_masked.reshape(len(X_val_masked),   1, 1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ac2b8f1-9aa1-4ffe-bdf7-6f26e0165392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data loaders for the masked data  \n",
    "y_train_masked = y_train.clone()\n",
    "y_val_masked   = y_val.clone()\n",
    "\n",
    "X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
    "X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n",
    "\n",
    "#creating the masked dataset\n",
    "train_dataset_masked = EEGDataset(X_train_masked, y_train_masked)\n",
    "val_dataset_masked   = EEGDataset(X_val_masked,   y_val_masked)\n",
    "\n",
    "#dataLoaders\n",
    "train_loader_masked = DataLoader(train_dataset_masked, batch_size=32, shuffle=True)\n",
    "val_loader_masked   = DataLoader(val_dataset_masked,   batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c458a18e-a926-4267-8f64-e95f767ffc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/p9h5rbz108g50kmwc52zw9fr0000gn/T/ipykernel_4193/922651201.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
      "/var/folders/jb/p9h5rbz108g50kmwc52zw9fr0000gn/T/ipykernel_4193/922651201.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# converting masked data to tensors \n",
    "X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
    "y_train_masked = y_train.clone()      # same labels\n",
    "X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n",
    "y_val_masked   = y_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0737075c-c40b-45ad-a1f3-623bf5f8e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatging new dataset and dataloaders for the data \n",
    "train_dataset_masked = EEGDataset(X_train_masked, y_train_masked)\n",
    "val_dataset_masked   = EEGDataset(X_val_masked,   y_val_masked)\n",
    "\n",
    "train_loader_masked = DataLoader(train_dataset_masked, batch_size=32, shuffle=True)\n",
    "val_loader_masked   = DataLoader(val_dataset_masked,   batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e665442d-fbc3-45d5-a729-29f2af022e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASKED TRAIN BATCH: torch.Size([32, 1, 1140])\n",
      "MASKED VAL BATCH: torch.Size([32, 1, 1140])\n"
     ]
    }
   ],
   "source": [
    "# Checking shapes for troubleshooting purposes \n",
    "for xb, yb in train_loader_masked:\n",
    "    print(\"MASKED TRAIN BATCH:\", xb.shape)\n",
    "    break\n",
    "\n",
    "for xb, yb in val_loader_masked:\n",
    "    print(\"MASKED VAL BATCH:\", xb.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eec001-e79a-4d14-ace3-0712b7d4685b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1Dimentional implemeentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "516c4d5d-65d2-489c-9927-4480b69d43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eeg net 1Dimentional implemeentation \n",
    "\n",
    "class EEGNet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int = 2,\n",
    "        Chans: int = 1,        # number of input channels\n",
    "        Samples: int = 1140,   # length of the time series\n",
    "        F1: int = 8,\n",
    "        D: int = 2,\n",
    "        kernel_length: int = 64,\n",
    "        dropout: float = 0.25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.Chans = Chans\n",
    "        self.Samples = Samples\n",
    "\n",
    "        # 1) Temporal convolution\n",
    "        self.conv_temporal = nn.Conv1d(\n",
    "            in_channels=Chans,\n",
    "            out_channels=F1,\n",
    "            kernel_size=kernel_length,\n",
    "            padding=kernel_length // 2,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(F1)\n",
    "\n",
    "        # 2) Depthwise convolution\n",
    "        #    Each filter operates on its own channel (groups=F1), multiplied by D\n",
    "        self.conv_depthwise = nn.Conv1d(\n",
    "            in_channels=F1,\n",
    "            out_channels=F1 * D,\n",
    "            kernel_size=kernel_length,\n",
    "            padding=kernel_length // 2,\n",
    "            groups=F1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(F1 * D)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 3) Separable convolution\n",
    "        #    depthwise (groups=F1*D) + pointwise (1x1 conv)\n",
    "        self.conv_separable_depth = nn.Conv1d(\n",
    "            in_channels=F1 * D,\n",
    "            out_channels=F1 * D,\n",
    "            kernel_size=16,\n",
    "            padding=16 // 2,\n",
    "            groups=F1 * D,\n",
    "            bias=False\n",
    "        )\n",
    "        self.conv_separable_point = nn.Conv1d(\n",
    "            in_channels=F1 * D,\n",
    "            out_channels=F1 * D * 2,  # F2 = 2 * F1 * D\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(F1 * D * 2)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=8)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 4) Global average pooling + classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(F1 * D * 2, n_classes)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1, 1140)\n",
    "        \"\"\"\n",
    "        # 1) Temporal conv\n",
    "        x = self.conv_temporal(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu(x)\n",
    "\n",
    "        # 2) Depthwise conv\n",
    "        x = self.conv_depthwise(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 3) Separable conv\n",
    "        x = self.conv_separable_depth(x)\n",
    "        x = self.conv_separable_point(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 4) Global pooling + classifier\n",
    "        x = self.global_pool(x)     # (batch, channels, 1)\n",
    "        x = x.squeeze(-1)           # (batch, channels)\n",
    "        logits = self.classifier(x) # (batch, n_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b390c26-ae56-42f2-a9a9-7271df765a6c",
   "metadata": {},
   "source": [
    "## Training both datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55dce9b1-3324-4789-b9b9-1f24614eb87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.6920 Acc: 0.5440 | Val Loss: 0.6794 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.6922 Acc: 0.5439 | Val Loss: 0.6784 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.6924 Acc: 0.5433 | Val Loss: 0.6786 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.6918 Acc: 0.5442 | Val Loss: 0.6789 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.6922 Acc: 0.5436 | Val Loss: 0.6785 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.6923 Acc: 0.5439 | Val Loss: 0.6779 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.6922 Acc: 0.5435 | Val Loss: 0.6782 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.6922 Acc: 0.5432 | Val Loss: 0.6791 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.6921 Acc: 0.5440 | Val Loss: 0.6793 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.6922 Acc: 0.5434 | Val Loss: 0.6783 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.6922 Acc: 0.5432 | Val Loss: 0.6782 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Train EEgnet on raw data \u001b[39;00m\n\u001b[1;32m      2\u001b[0m eegnet_raw \u001b[38;5;241m=\u001b[39m EEGNet1D(n_classes\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, Chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, Samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1140\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m history_raw \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meegnet_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Saving for Saliency later\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(history_raw\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_EEGNet.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr)\u001b[0m\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m:  [],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:   [],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m:    []\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 11\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m     16\u001b[0m         model, val_loader, criterion, device\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[44], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 4. Compute gradients\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 5. Update weights\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train EEgnet on raw data \n",
    "eegnet_raw = EEGNet1D(n_classes= 2, Chans=1, Samples=1140).to(device)\n",
    "\n",
    "history_raw = train_model(\n",
    "    model=eegnet_raw,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "torch.save(history_raw.state_dict(), \"raw_EEGNet.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"raw_EEGNet.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65f950cc-217a-403c-85b7-b31b2f8f953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Train EEg net on masked data \u001b[39;00m\n\u001b[1;32m      2\u001b[0m eegnet_masked \u001b[38;5;241m=\u001b[39m EEGNet1D(n_classes\u001b[38;5;241m=\u001b[39m X_train_masked\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, Samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1140\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m history_masked \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meegnet_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Saving for Saliency later\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(history_masked\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_EEGNet.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr)\u001b[0m\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m:  [],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:   [],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m:    []\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 11\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m     16\u001b[0m         model, val_loader, criterion, device\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[44], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 2. Compute loss\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 3. Zero out old gradients\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 1 is out of bounds."
     ]
    }
   ],
   "source": [
    "#Train EEg net on masked data \n",
    "eegnet_masked = EEGNet1D(n_classes= 2, Chans=1, Samples=1140).to(device)\n",
    "\n",
    "history_masked = train_model(\n",
    "    model=eegnet_masked,\n",
    "    train_loader=train_loader_masked,\n",
    "    val_loader=val_loader_masked,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "torch.save(history_masked.state_dict(), \"masked_EEGNet.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"masked_EEGNet.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3db5c7-eb60-41bf-b12e-79d57eed2e9d",
   "metadata": {},
   "source": [
    "## Simple evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62917346-36b6-4d19-a055-0a81b1514b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_loaded = ResNet1D(n_channels=1, n_classes=2).to(device)\n",
    "resnet_loaded.load_state_dict(torch.load(\"resnet_eeg.pth\", map_location=device))\n",
    "resnet_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7ec9200-2cee-49d3-994c-e16310e74b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75afc4be-d147-4cc4-b6c2-b218ed920c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_resnet = evaluate_model(resnet_loaded, val_loader, device)\n",
    "acc_eegnet_raw = evaluate_model(eegnet_raw_loaded, val_loader, device)\n",
    "acc_eegnet_masked = evaluate_model(eegnet_masked_loaded, val_loader_masked, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3974493b-cacf-40a4-8f58-409306884f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40534262485482\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy acore for the resnet arch\", acc_resnet)\n",
    "print(\"Accuracy acore for the Raw eegnet data\",acc_eegnet_raw)\n",
    "print(\"Accuracy acore for the masked eegnet data\",acc_eegnet_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b05c7-b1b2-4d10-a7c9-63a25937d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
