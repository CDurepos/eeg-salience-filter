{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8871c87-3e58-4abb-9dc8-1041537ffc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65e332-062b-44c0-9dc8-3e20b1717d15",
   "metadata": {},
   "source": [
    "## Building the 1D-CNN with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f905e2-1ad6-412b-83ed-fc258a34e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # First conv layer (uses passed in in_channels, NOT hard-coded)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Downsample (skip connection) if needed\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Apply skip path if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df5c6dc-44e8-4e92-ab6d-4e5966efbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution \"stem\"\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual stages\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling over time dimension\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # output: (batch, channels, 1)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc = nn.Linear(512, n_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        # First block may change channels/stride\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride=stride))\n",
    "        # Remaining blocks keep same channels/stride=1\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, n_channels, n_times)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)  # shape: (batch, 64, T1)\n",
    "        x = self.layer2(x)  # shape: (batch, 128, T2)\n",
    "        x = self.layer3(x)  # shape: (batch, 256, T3)\n",
    "        x = self.layer4(x)  # shape: (batch, 512, T4)\n",
    "        \n",
    "        # Global average pooling: average over time dimension\n",
    "        x = self.global_pool(x)  # (batch, 512, 1)\n",
    "        x = x.squeeze(-1)        # (batch, 512)\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.fc(x)      # (batch, n_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ac935-d22c-4a3f-9620-faf71cb04d70",
   "metadata": {},
   "source": [
    "## Building the training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dd283f-4d57-49e7-af87-0930a2cc895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.float()\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40692463-afd7-4c4e-92f3-0c9328b2797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and Y based on subjects \n",
    "X_raw = np.load(\"../data/X_tqwt_wpd.npy\")\n",
    "y_raw = np.load(\"../data/y_labels.npy\")\n",
    "\n",
    "subject_ids = np.load(\"../data/subject_ids.npy\", allow_pickle=True)\n",
    "unique_subs = np.unique(subject_ids)\n",
    "\n",
    "# Not required \n",
    "# print(\"Number of subjects:\", len(unique_subs))\n",
    "# print(\"Subject IDs:\", unique_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9f9f78-6f5c-4dc4-aa40-55f934ada254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subs, val_subs = train_test_split(\n",
    "    unique_subs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5690d141-40d0-4331-8883-4776b3580e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically ensures that there is no leakage \n",
    "\n",
    "train_mask = np.isin(subject_ids, train_subs)\n",
    "val_mask = np.isin(subject_ids, val_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf7a0c3-9481-456a-96f3-45127677475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/X_tqwt_wpd.npy\")\n",
    "y = np.load(\"../data/y_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec696dcc-f884-4cc8-8152-e061795ad6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (16749, 1140)\n",
      "y shape: (16749,)\n",
      "subject_ids shape: (16749,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"subject_ids shape:\", subject_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b08355-f8a0-4f6a-a63e-ec74e6da25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "X1 = X[train_mask]\n",
    "y1 = y[train_mask]\n",
    "\n",
    "X_val_np = X[val_mask]\n",
    "y_val_np = y[val_mask]\n",
    "\n",
    "\n",
    "X_train_np = torch.tensor(X1, dtype=torch.float32)\n",
    "y_train_np = torch.tensor(y1, dtype=torch.long)\n",
    "\n",
    "X_val_np = torch.tensor(X_val_np, dtype=torch.float32)\n",
    "y_val_np = torch.tensor(y_val_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5e1e8-4c27-4146-a491-2b90e849c5df",
   "metadata": {},
   "source": [
    "### Lets save the files once... No need to do this if you already have the files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11e9d3b-797a-44f3-a4a2-3fdeb435009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../data/X_train.npy\", X_train_np)\n",
    "# np.save(\"../data/y_train.npy\", y_train_np)\n",
    "# np.save(\"../data/X_val.npy\", X_val_np)\n",
    "# np.save(\"../data/y_val.npy\", y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfbc534-28f3-416e-ba04-207103589d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping input to DataLoader:\n",
      "(13305, 1140)\n",
      "Raw X_val shape: (3444, 1140)\n",
      "after reshaping input to DataLoader:\n",
      "(13305, 1, 1140)\n",
      "Raw X_val shape: (3444, 1, 1140)\n"
     ]
    }
   ],
   "source": [
    "# even though they were converted to tensors before saving they need to be converted back becsuse when loading them with np it becomes an array again.\n",
    "X_train = np.load(\"../data/X_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "X_val   = np.load(\"../data/X_val.npy\")\n",
    "y_val   = np.load(\"../data/y_val.npy\")\n",
    "\n",
    "\n",
    "print(\"Before reshaping input to DataLoader:\")\n",
    "print(X_train.shape)\n",
    "print(\"Raw X_val shape:\",   X_val.shape)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), 1, -1)\n",
    "X_val   = X_val.reshape(len(X_val), 1, -1)\n",
    "\n",
    "print(\"after reshaping input to DataLoader:\")\n",
    "print(X_train.shape)\n",
    "print(\"Raw X_val shape:\",   X_val.shape)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b421ed00-fbb9-4374-a17a-2d95f25402a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataloaders \n",
    "\n",
    "train_dataset = EEGDataset(X_train, y_train)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                           \n",
    "val_dataset = EEGDataset(X_val, y_val)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51c33ac7-630b-4419-8395-d28aabd0a875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN BATCH SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL BATCH SHAPE: torch.Size([32, 1, 1140])\n",
      "X_train shape from file: torch.Size([13305, 1, 1140])\n",
      "X_val shape from file: torch.Size([3444, 1, 1140])\n",
      "First element type: <class 'torch.Tensor'>\n",
      "First element shape: torch.Size([1, 1140])\n"
     ]
    }
   ],
   "source": [
    "# Safety Check\n",
    "for xb, yb in train_loader:\n",
    "    print(\"TRAIN BATCH SHAPE:\", xb.shape)\n",
    "    break\n",
    "\n",
    "for xb, yb in val_loader:\n",
    "    print(\"VAL BATCH SHAPE:\", xb.shape)\n",
    "    break\n",
    "\n",
    "print(\"X_train shape from file:\", X_train.shape)\n",
    "print(\"X_val shape from file:\", X_val.shape)\n",
    "\n",
    "print(\"First element type:\", type(X_train[0]))\n",
    "print(\"First element shape:\", getattr(X_train[0], 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34bbd68f-ff07-4eaf-99be-f5399f1a8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialze the CNN\n",
    "\n",
    "n_channels = 1      # or however many EEG channels we have\n",
    "n_classes = 2        # ADHD vs Control\n",
    "\n",
    "model = ResNet1D(n_channels=n_channels, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a0a39a-01c4-4250-bb56-3dc86f6612ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letting the computer know what piece of hardware to run the training \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841ae2f-ec04-4a17-aa17-8167e424bb72",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bde9f48-121a-4de8-b6ce-b4e3e3d0c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy calculations\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == y).sum().item()\n",
    "    total = y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba2ce2b0-c52d-4157-8d8e-375695b232aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # put model in \"training mode\"\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        print(\"TRAIN LOOP SHAPE:\", X.shape)\n",
    "        break\n",
    "        \n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        # 2. Compute loss\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # 3. Zero out old gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training accuracy & loss\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "#only run to check if functional. and last i ran it was functional\n",
    "#train_one_epoch(model, train_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28d89e14-74fc-4123-8d07-9fbf4abd071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid loop with no gradient Updates \n",
    "# Very similar to the training loop, except this one sets the model to eval and its accompanied by other\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Do NOT track gradients\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            print(\"VAL LOOP SHAPE:\", X.shape)\n",
    "            break\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "#only run to check if functional. and last i ran it was functional\n",
    "#validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "125ef7cb-8a43-45fb-a02f-3e43623581bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=20, lr=1e-3):\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\":  [],\n",
    "        \"val_loss\":   [],\n",
    "        \"val_acc\":    []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6f6f3-fa2b-4eb2-9cf6-48c56761a33d",
   "metadata": {},
   "source": [
    "## Training the architectiure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc8ad2d7-5e01-4200-8a54-97c181d610f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared. Available: 50.89 GB\n",
      "Model created on cuda\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.7207 Acc: 0.5222 | Val Loss: 0.7412 Acc: 0.4605\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.7213 Acc: 0.5199 | Val Loss: 0.7535 Acc: 0.4579\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.7214 Acc: 0.5158 | Val Loss: 0.7279 Acc: 0.4326\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.7210 Acc: 0.5180 | Val Loss: 0.7382 Acc: 0.4506\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.7207 Acc: 0.5235 | Val Loss: 0.7314 Acc: 0.4576\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.7206 Acc: 0.5200 | Val Loss: 0.7365 Acc: 0.4126\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.7207 Acc: 0.5260 | Val Loss: 0.7603 Acc: 0.4652\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.7205 Acc: 0.5200 | Val Loss: 0.7362 Acc: 0.4474\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.7215 Acc: 0.5183 | Val Loss: 0.7222 Acc: 0.4065\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.7199 Acc: 0.5135 | Val Loss: 0.7333 Acc: 0.4506\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.7206 Acc: 0.5255 | Val Loss: 0.7446 Acc: 0.4562\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 12/20 | Train Loss: 0.7213 Acc: 0.5159 | Val Loss: 0.7388 Acc: 0.4593\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 13/20 | Train Loss: 0.7192 Acc: 0.5232 | Val Loss: 0.7596 Acc: 0.4698\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 14/20 | Train Loss: 0.7197 Acc: 0.5200 | Val Loss: 0.7223 Acc: 0.4068\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 15/20 | Train Loss: 0.7193 Acc: 0.5174 | Val Loss: 0.7603 Acc: 0.4805\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 16/20 | Train Loss: 0.7217 Acc: 0.5202 | Val Loss: 0.7275 Acc: 0.4143\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 17/20 | Train Loss: 0.7191 Acc: 0.5227 | Val Loss: 0.7336 Acc: 0.4434\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 18/20 | Train Loss: 0.7207 Acc: 0.5215 | Val Loss: 0.7539 Acc: 0.4779\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 19/20 | Train Loss: 0.7211 Acc: 0.5192 | Val Loss: 0.7371 Acc: 0.4486\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 20/20 | Train Loss: 0.7204 Acc: 0.5230 | Val Loss: 0.7588 Acc: 0.4654\n",
      "Model saved to ../models/resnet_eeg.pth\n"
     ]
    }
   ],
   "source": [
    "#Training the resnet architeciure on Raw data \n",
    "\n",
    "# Clear GPU cache before creating model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Try to create model on GPU, fallback to CPU if OOM\n",
    "try:\n",
    "    resnet_raw = ResNet1D(n_channels=X_train.shape[1], n_classes=2).to(device)\n",
    "    print(f\"Model created on {device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower() or \"cuda\" in str(e).lower():\n",
    "        print(f\"CUDA OOM error. Falling back to CPU...\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "        resnet_raw = ResNet1D(n_channels=X_train.shape[1], n_classes=2).to(device)\n",
    "        print(f\"Model created on CPU\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "history_resnet_raw = train_model(\n",
    "    model=resnet_raw,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "torch.save(resnet_raw.state_dict(), \"../models/resnet_eeg.pth\")\n",
    "print(\"Model saved to ../models/resnet_eeg.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"resnet_eeg.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a826eeb-3d56-4f96-9316-d40a3b552279",
   "metadata": {},
   "source": [
    "## Compute saliency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd681b-62eb-441e-acc1-b271245ca707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(model, X_batch, y_batch, device):\n",
    "    model.eval()\n",
    "    # Ensure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_batch = X_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    X_batch.requires_grad = True  # Enable gradient wrt input\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(X_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient wrt input\n",
    "    saliency = X_batch.grad.detach().abs()  # absolute gradient\n",
    "\n",
    "    return saliency.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d1121-6e74-4438-a419-6499e472b3ca",
   "metadata": {},
   "source": [
    "## Global saliency vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "514d45ff-2a2f-4957-b90b-46310c9822b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m all_saliencies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Xb, yb \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m----> 4\u001b[0m     sal \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_saliency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     all_saliencies\u001b[38;5;241m.\u001b[39mappend(sal)\n\u001b[1;32m      7\u001b[0m all_saliencies \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_saliencies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# (N_val, 1, 1140)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m, in \u001b[0;36mcompute_saliency\u001b[0;34m(model, X_batch, y_batch, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m X_batch\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Enable gradient wrt input\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y_batch)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mResNet1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# x shape: (batch, n_channels, n_times)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:371\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:366\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    356\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    365\u001b[0m     )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "all_saliencies = []\n",
    "\n",
    "for Xb, yb in val_loader:\n",
    "    sal = compute_saliency(model, Xb, yb, device)\n",
    "    all_saliencies.append(sal)\n",
    "\n",
    "all_saliencies = np.concatenate(all_saliencies, axis=0)   # (N_val, 1, 1140)\n",
    "\n",
    "# Average across samples and channel\n",
    "global_saliency = all_saliencies.mean(axis=0).squeeze(0)  # (1140,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b5fb6-161c-41d7-87d0-703b3c5d30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "global_saliency = global_saliency / global_saliency.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4488fd-d338-4c98-9056-f4b8fa9328a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: (1140,)\n"
     ]
    }
   ],
   "source": [
    "# Ceating a binary mask \n",
    "threshold = 0.2  # keep top 80% important features or however much we wanna keep\n",
    "mask = (global_saliency > threshold).astype(np.float32) # does this not return a boolean ?? \n",
    "\n",
    "\n",
    "print(\"Mask shape:\", mask.shape)  # (1140,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a499c-436a-447d-b557-dfe9bb32fcef",
   "metadata": {},
   "source": [
    "## Handling the Masked data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6479-638d-420d-af92-c790d48eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying saliency mask to the dataset, actually just uses simple multiplication\n",
    "\n",
    "# Convert tensors back to numpy so we can multiply with mask\n",
    "X_train_np = X_train.cpu().numpy().reshape(len(X_train), 1140)\n",
    "X_val_np   = X_val.cpu().numpy().reshape(len(X_val), 1140)\n",
    "\n",
    "#applying the mask\n",
    "X_train_masked = X_train_np * mask\n",
    "X_val_masked   = X_val_np * mask\n",
    "\n",
    "#reshaping back to conv1d format \n",
    "X_train_masked = X_train_masked.reshape(len(X_train_masked), 1, 1140)\n",
    "X_val_masked   = X_val_masked.reshape(len(X_val_masked),   1, 1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2b8f1-9aa1-4ffe-bdf7-6f26e0165392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data loaders for the masked data  \n",
    "y_train_masked = y_train.clone()\n",
    "y_val_masked   = y_val.clone()\n",
    "\n",
    "X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
    "X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n",
    "\n",
    "#creating the masked dataset\n",
    "train_dataset_masked = EEGDataset(X_train_masked, y_train_masked)\n",
    "val_dataset_masked   = EEGDataset(X_val_masked,   y_val_masked)\n",
    "\n",
    "#dataLoaders\n",
    "train_loader_masked = DataLoader(train_dataset_masked, batch_size=32, shuffle=True)\n",
    "val_loader_masked   = DataLoader(val_dataset_masked,   batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458a18e-a926-4267-8f64-e95f767ffc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3192365/922651201.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
      "/tmp/ipykernel_3192365/922651201.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# converting masked data to tensors \n",
    "X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
    "y_train_masked = y_train.clone()      # same labels\n",
    "X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n",
    "y_val_masked   = y_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737075c-c40b-45ad-a1f3-623bf5f8e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatging new dataset and dataloaders for the data \n",
    "train_dataset_masked = EEGDataset(X_train_masked, y_train_masked)\n",
    "val_dataset_masked   = EEGDataset(X_val_masked,   y_val_masked)\n",
    "\n",
    "train_loader_masked = DataLoader(train_dataset_masked, batch_size=32, shuffle=True)\n",
    "val_loader_masked   = DataLoader(val_dataset_masked,   batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665442d-fbc3-45d5-a729-29f2af022e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASKED TRAIN BATCH: torch.Size([32, 1, 1140])\n",
      "MASKED VAL BATCH: torch.Size([32, 1, 1140])\n"
     ]
    }
   ],
   "source": [
    "# Checking shapes for troubleshooting purposes \n",
    "for xb, yb in train_loader_masked:\n",
    "    print(\"MASKED TRAIN BATCH:\", xb.shape)\n",
    "    break\n",
    "\n",
    "for xb, yb in val_loader_masked:\n",
    "    print(\"MASKED VAL BATCH:\", xb.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eec001-e79a-4d14-ace3-0712b7d4685b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1Dimentional implemeentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c4d5d-65d2-489c-9927-4480b69d43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eeg net 1Dimentional implemeentation \n",
    "\n",
    "class EEGNet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int = 2,\n",
    "        Chans: int = 1,        # number of input channels\n",
    "        Samples: int = 1140,   # length of the time series\n",
    "        F1: int = 8,\n",
    "        D: int = 2,\n",
    "        kernel_length: int = 64,\n",
    "        dropout: float = 0.25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.Chans = Chans\n",
    "        self.Samples = Samples\n",
    "\n",
    "        # 1) Temporal convolution\n",
    "        self.conv_temporal = nn.Conv1d(\n",
    "            in_channels=Chans,\n",
    "            out_channels=F1,\n",
    "            kernel_size=kernel_length,\n",
    "            padding=kernel_length // 2,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(F1)\n",
    "\n",
    "        # 2) Depthwise convolution\n",
    "        #    Each filter operates on its own channel (groups=F1), multiplied by D\n",
    "        self.conv_depthwise = nn.Conv1d(\n",
    "            in_channels=F1,\n",
    "            out_channels=F1 * D,\n",
    "            kernel_size=kernel_length,\n",
    "            padding=kernel_length // 2,\n",
    "            groups=F1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(F1 * D)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 3) Separable convolution\n",
    "        #    depthwise (groups=F1*D) + pointwise (1x1 conv)\n",
    "        self.conv_separable_depth = nn.Conv1d(\n",
    "            in_channels=F1 * D,\n",
    "            out_channels=F1 * D,\n",
    "            kernel_size=16,\n",
    "            padding=16 // 2,\n",
    "            groups=F1 * D,\n",
    "            bias=False\n",
    "        )\n",
    "        self.conv_separable_point = nn.Conv1d(\n",
    "            in_channels=F1 * D,\n",
    "            out_channels=F1 * D * 2,  # F2 = 2 * F1 * D\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(F1 * D * 2)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=8)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 4) Global average pooling + classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(F1 * D * 2, n_classes)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1, 1140)\n",
    "        \"\"\"\n",
    "        # 1) Temporal conv\n",
    "        x = self.conv_temporal(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu(x)\n",
    "\n",
    "        # 2) Depthwise conv\n",
    "        x = self.conv_depthwise(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 3) Separable conv\n",
    "        x = self.conv_separable_depth(x)\n",
    "        x = self.conv_separable_point(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 4) Global pooling + classifier\n",
    "        x = self.global_pool(x)     # (batch, channels, 1)\n",
    "        x = x.squeeze(-1)           # (batch, channels)\n",
    "        logits = self.classifier(x) # (batch, n_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b390c26-ae56-42f2-a9a9-7271df765a6c",
   "metadata": {},
   "source": [
    "## Training both datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dce9b1-3324-4789-b9b9-1f24614eb87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.7134 Acc: 0.4570 | Val Loss: 0.7469 Acc: 0.3969\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.7141 Acc: 0.4577 | Val Loss: 0.7567 Acc: 0.3952\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.7141 Acc: 0.4572 | Val Loss: 0.7367 Acc: 0.4074\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.7142 Acc: 0.4577 | Val Loss: 0.7419 Acc: 0.3966\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.7140 Acc: 0.4571 | Val Loss: 0.7362 Acc: 0.4117\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.7142 Acc: 0.4557 | Val Loss: 0.7635 Acc: 0.3902\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.7141 Acc: 0.4588 | Val Loss: 0.7466 Acc: 0.3990\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.7138 Acc: 0.4555 | Val Loss: 0.7373 Acc: 0.3998\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.7140 Acc: 0.4538 | Val Loss: 0.7417 Acc: 0.4080\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.7142 Acc: 0.4579 | Val Loss: 0.7428 Acc: 0.3981\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.7144 Acc: 0.4538 | Val Loss: 0.7374 Acc: 0.4074\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 12/20 | Train Loss: 0.7141 Acc: 0.4558 | Val Loss: 0.7391 Acc: 0.4117\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 13/20 | Train Loss: 0.7143 Acc: 0.4582 | Val Loss: 0.7419 Acc: 0.4030\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 14/20 | Train Loss: 0.7143 Acc: 0.4552 | Val Loss: 0.7439 Acc: 0.4007\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 15/20 | Train Loss: 0.7138 Acc: 0.4557 | Val Loss: 0.7419 Acc: 0.4030\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 16/20 | Train Loss: 0.7146 Acc: 0.4567 | Val Loss: 0.7468 Acc: 0.3958\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 17/20 | Train Loss: 0.7140 Acc: 0.4588 | Val Loss: 0.7490 Acc: 0.3972\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 18/20 | Train Loss: 0.7140 Acc: 0.4577 | Val Loss: 0.7463 Acc: 0.3984\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 19/20 | Train Loss: 0.7145 Acc: 0.4589 | Val Loss: 0.7381 Acc: 0.4030\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 20/20 | Train Loss: 0.7137 Acc: 0.4550 | Val Loss: 0.7507 Acc: 0.3926\n",
      "Model saved to ../models/raw_EEGNet.pth\n"
     ]
    }
   ],
   "source": [
    "#Train EEgnet on raw data \n",
    "eegnet_raw = EEGNet1D(n_classes= 2, Chans=1, Samples=1140).to(device)\n",
    "\n",
    "history_raw = train_model(\n",
    "    model=eegnet_raw,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "torch.save(eegnet_raw.state_dict(), \"../models/raw_EEGNet.pth\")\n",
    "print(\"Model saved to ../models/raw_EEGNet.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"raw_EEGNet.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f950cc-217a-403c-85b7-b31b2f8f953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.6935 Acc: 0.5361 | Val Loss: 0.6766 Acc: 0.6060\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.6932 Acc: 0.5392 | Val Loss: 0.6782 Acc: 0.6028\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.6930 Acc: 0.5387 | Val Loss: 0.6856 Acc: 0.5947\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.6930 Acc: 0.5394 | Val Loss: 0.6870 Acc: 0.5923\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.6930 Acc: 0.5393 | Val Loss: 0.6903 Acc: 0.5932\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.6937 Acc: 0.5396 | Val Loss: 0.6833 Acc: 0.5918\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.6932 Acc: 0.5400 | Val Loss: 0.6851 Acc: 0.5920\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.6928 Acc: 0.5402 | Val Loss: 0.6856 Acc: 0.5941\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.6932 Acc: 0.5403 | Val Loss: 0.6918 Acc: 0.5938\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.6930 Acc: 0.5387 | Val Loss: 0.6798 Acc: 0.5993\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.6931 Acc: 0.5392 | Val Loss: 0.6863 Acc: 0.5932\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 12/20 | Train Loss: 0.6935 Acc: 0.5381 | Val Loss: 0.6876 Acc: 0.5923\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 13/20 | Train Loss: 0.6936 Acc: 0.5380 | Val Loss: 0.6886 Acc: 0.5941\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 14/20 | Train Loss: 0.6933 Acc: 0.5412 | Val Loss: 0.6864 Acc: 0.5932\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 15/20 | Train Loss: 0.6930 Acc: 0.5396 | Val Loss: 0.6860 Acc: 0.5932\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 16/20 | Train Loss: 0.6932 Acc: 0.5396 | Val Loss: 0.6846 Acc: 0.5949\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 17/20 | Train Loss: 0.6932 Acc: 0.5399 | Val Loss: 0.6887 Acc: 0.5938\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 18/20 | Train Loss: 0.6934 Acc: 0.5388 | Val Loss: 0.6764 Acc: 0.6063\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 19/20 | Train Loss: 0.6931 Acc: 0.5410 | Val Loss: 0.6872 Acc: 0.5923\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 20/20 | Train Loss: 0.6936 Acc: 0.5379 | Val Loss: 0.6869 Acc: 0.5923\n",
      "Model saved to ../models/masked_EEGNet.pth\n"
     ]
    }
   ],
   "source": [
    "#Train EEg net on masked data \n",
    "eegnet_masked = EEGNet1D(n_classes= 2, Chans=1, Samples=1140).to(device)\n",
    "\n",
    "history_masked = train_model(\n",
    "    model=eegnet_masked,\n",
    "    train_loader=train_loader_masked,\n",
    "    val_loader=val_loader_masked,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "torch.save(eegnet_masked.state_dict(), \"../models/masked_EEGNet.pth\")\n",
    "print(\"Model saved to ../models/masked_EEGNet.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"masked_EEGNet.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3db5c7-eb60-41bf-b12e-79d57eed2e9d",
   "metadata": {},
   "source": [
    "## Simple evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62917346-36b6-4d19-a055-0a81b1514b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_loaded = ResNet1D(n_channels=1, n_classes=2).to(device)\n",
    "resnet_loaded.load_state_dict(torch.load(\"../models/resnet_eeg.pth\", map_location=device))\n",
    "resnet_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec9200-2cee-49d3-994c-e16310e74b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total   += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afc4be-d147-4cc4-b6c2-b218ed920c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_resnet = evaluate_model(resnet_loaded, val_loader, device)\n",
    "acc_eegnet_raw = evaluate_model(eegnet_raw, val_loader, device)\n",
    "acc_eegnet_masked = evaluate_model(eegnet_masked, val_loader_masked, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974493b-cacf-40a4-8f58-409306884f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy acore for the resnet arch:  0.39198606271777003\n",
      "Accuracy acore for the Raw eegnet data:  0.39256678281068524\n",
      "Accuracy acore for the masked eegnet data:  0.5923344947735192\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy acore for the resnet arch: \", acc_resnet)\n",
    "print(\"Accuracy acore for the Raw eegnet data: \",acc_eegnet_raw)\n",
    "print(\"Accuracy acore for the masked eegnet data: \",acc_eegnet_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb4c77",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba942b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for Keras EEGNet:\n",
      "  X_train: (13305, 1140) y_train: (13305,)\n",
      "  X_val: (3444, 1140) y_val: (3444,)\n",
      "Keras EEGNet input shapes:\n",
      "  X_train_eegnet: (13305, 1, 1140, 1) y_train_cat: (13305, 2)\n",
      "  X_val_eegnet: (3444, 1, 1140, 1) y_val_cat: (3444, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clayton.durepos/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764860521.373841 3067284 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46689 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:ac:00.0, compute capability: 8.6\n",
      "I0000 00:00:1764860521.375684 3067284 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46718 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:ca:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 10:02:03.843616: I external/local_xla/xla/service/service.cc:163] XLA service 0x7ee44c0048f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-04 10:02:03.843662: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-12-04 10:02:03.843670: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-12-04 10:02:03.899052: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-04 10:02:04.182870: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-12-04 10:02:04.564481: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1905', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1764860527.043760 3070221 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-12-04 10:02:08.634326: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1520', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-04 10:02:08.657878: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1520', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-04 10:02:08.663487: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1520', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-04 10:02:11.311136: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-04 10:02:11.330580: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-04 10:02:11.337638: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_185', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 - 9s - 43ms/step - accuracy: 0.5897 - loss: 0.6715 - val_accuracy: 0.6420 - val_loss: 0.6590\n",
      "Epoch 2/20\n",
      "208/208 - 1s - 6ms/step - accuracy: 0.6280 - loss: 0.6455 - val_accuracy: 0.5912 - val_loss: 0.6600\n",
      "Epoch 3/20\n",
      "208/208 - 1s - 6ms/step - accuracy: 0.6332 - loss: 0.6418 - val_accuracy: 0.5772 - val_loss: 0.6998\n",
      "Epoch 4/20\n",
      "208/208 - 1s - 6ms/step - accuracy: 0.6322 - loss: 0.6406 - val_accuracy: 0.5662 - val_loss: 0.6924\n",
      "Epoch 5/20\n",
      "208/208 - 1s - 7ms/step - accuracy: 0.6433 - loss: 0.6305 - val_accuracy: 0.5581 - val_loss: 0.7023\n",
      "Epoch 6/20\n",
      "208/208 - 1s - 7ms/step - accuracy: 0.6452 - loss: 0.6292 - val_accuracy: 0.5674 - val_loss: 0.6861\n",
      "Epoch 7/20\n",
      "208/208 - 2s - 7ms/step - accuracy: 0.6420 - loss: 0.6330 - val_accuracy: 0.5819 - val_loss: 0.6751\n",
      "Epoch 8/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6486 - loss: 0.6285 - val_accuracy: 0.5302 - val_loss: 0.7142\n",
      "Epoch 9/20\n",
      "208/208 - 2s - 7ms/step - accuracy: 0.6484 - loss: 0.6256 - val_accuracy: 0.5764 - val_loss: 0.6867\n",
      "Epoch 10/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6539 - loss: 0.6244 - val_accuracy: 0.5688 - val_loss: 0.7067\n",
      "Epoch 11/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6522 - loss: 0.6229 - val_accuracy: 0.5828 - val_loss: 0.6722\n",
      "Epoch 12/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6506 - loss: 0.6238 - val_accuracy: 0.5447 - val_loss: 0.7390\n",
      "Epoch 13/20\n",
      "208/208 - 2s - 7ms/step - accuracy: 0.6545 - loss: 0.6220 - val_accuracy: 0.5813 - val_loss: 0.6640\n",
      "Epoch 14/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6480 - loss: 0.6229 - val_accuracy: 0.5546 - val_loss: 0.6938\n",
      "Epoch 15/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6504 - loss: 0.6208 - val_accuracy: 0.5804 - val_loss: 0.6736\n",
      "Epoch 16/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6594 - loss: 0.6204 - val_accuracy: 0.5886 - val_loss: 0.6861\n",
      "Epoch 17/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6587 - loss: 0.6179 - val_accuracy: 0.5427 - val_loss: 0.7193\n",
      "Epoch 18/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6582 - loss: 0.6192 - val_accuracy: 0.5761 - val_loss: 0.6864\n",
      "Epoch 19/20\n",
      "208/208 - 2s - 7ms/step - accuracy: 0.6607 - loss: 0.6179 - val_accuracy: 0.5679 - val_loss: 0.7183\n",
      "Epoch 20/20\n",
      "208/208 - 2s - 8ms/step - accuracy: 0.6609 - loss: 0.6160 - val_accuracy: 0.5787 - val_loss: 0.6976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EEGNet (raw) final val accuracy: 0.5786875486373901\n",
      "Saved raw EEGNet model to ../models/eegnet_raw_keras.h5\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488d84e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e87663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcee47",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461f1e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'global_saliency' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 4) Create binary mask from saliency map\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Use percentile-based threshold (keep top 80% most salient features)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m threshold_percentile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Keep top 80% (bottom 20% threshold)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m threshold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(\u001b[43mglobal_saliency\u001b[49m, threshold_percentile)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create binary mask: 1 for important features, 0 for less important\u001b[39;00m\n\u001b[1;32m     10\u001b[0m mask \u001b[38;5;241m=\u001b[39m (global_saliency \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'global_saliency' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769963b7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0867775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec10124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a548f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a89ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-salience-filter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
