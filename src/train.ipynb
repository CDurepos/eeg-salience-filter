{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8871c87-3e58-4abb-9dc8-1041537ffc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65e332-062b-44c0-9dc8-3e20b1717d15",
   "metadata": {},
   "source": [
    "## Building the 1D-CNN with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f905e2-1ad6-412b-83ed-fc258a34e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # First conv layer (uses passed in in_channels, NOT hard-coded)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Downsample (skip connection) if needed\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Apply skip path if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2df5c6dc-44e8-4e92-ab6d-4e5966efbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution \"stem\"\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual stages\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling over time dimension\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # output: (batch, channels, 1)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc = nn.Linear(512, n_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        # First block may change channels/stride\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride=stride))\n",
    "        # Remaining blocks keep same channels/stride=1\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, n_channels, n_times)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)  # shape: (batch, 64, T1)\n",
    "        x = self.layer2(x)  # shape: (batch, 128, T2)\n",
    "        x = self.layer3(x)  # shape: (batch, 256, T3)\n",
    "        x = self.layer4(x)  # shape: (batch, 512, T4)\n",
    "        \n",
    "        # Global average pooling: average over time dimension\n",
    "        x = self.global_pool(x)  # (batch, 512, 1)\n",
    "        x = x.squeeze(-1)        # (batch, 512)\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.fc(x)      # (batch, n_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ac935-d22c-4a3f-9620-faf71cb04d70",
   "metadata": {},
   "source": [
    "## Building the training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99dd283f-4d57-49e7-af87-0930a2cc895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.float()\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40692463-afd7-4c4e-92f3-0c9328b2797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and Y based on subjects \n",
    "X_raw = np.load(\"../data/X_tqwt_wpd.npy\")\n",
    "y_raw = np.load(\"../data/y_labels.npy\")\n",
    "\n",
    "subject_ids = np.load(\"../data/subject_ids.npy\", allow_pickle=True)\n",
    "unique_subs = np.unique(subject_ids)\n",
    "\n",
    "# Not required \n",
    "# print(\"Number of subjects:\", len(unique_subs))\n",
    "# print(\"Subject IDs:\", unique_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc9f9f78-6f5c-4dc4-aa40-55f934ada254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subs, val_subs = train_test_split(\n",
    "    unique_subs,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5690d141-40d0-4331-8883-4776b3580e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically ensures that there is no leakage \n",
    "\n",
    "train_mask = np.isin(subject_ids, train_subs)\n",
    "val_mask = np.isin(subject_ids, val_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cf7a0c3-9481-456a-96f3-45127677475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/X_tqwt_wpd.npy\")\n",
    "y = np.load(\"../data/y_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec696dcc-f884-4cc8-8152-e061795ad6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (16749, 1140)\n",
      "y shape: (16749,)\n",
      "subject_ids shape: (16749,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"subject_ids shape:\", subject_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43b08355-f8a0-4f6a-a63e-ec74e6da25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "X1 = X[train_mask]\n",
    "y1 = y[train_mask]\n",
    "\n",
    "X_val_np = X[val_mask]\n",
    "y_val_np = y[val_mask]\n",
    "\n",
    "\n",
    "X_train_np = torch.tensor(X1, dtype=torch.float32)\n",
    "y_train_np = torch.tensor(y1, dtype=torch.long)\n",
    "\n",
    "X_val_np = torch.tensor(X_val_np, dtype=torch.float32)\n",
    "y_val_np = torch.tensor(y_val_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5e1e8-4c27-4146-a491-2b90e849c5df",
   "metadata": {},
   "source": [
    "### Lets save the files once... No need to do this if you already have the files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11e9d3b-797a-44f3-a4a2-3fdeb435009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../data/X_train.npy\", X_train_np)\n",
    "np.save(\"../data/y_train.npy\", y_train_np)\n",
    "np.save(\"../data/X_val.npy\", X_val_np)\n",
    "np.save(\"../data/y_val.npy\", y_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbfbc534-28f3-416e-ba04-207103589d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping input to DataLoader:\n",
      "(13305, 1140)\n",
      "Raw X_val shape: (3444, 1140)\n",
      "after reshaping input to DataLoader:\n",
      "(13305, 1, 1140)\n",
      "Raw X_val shape: (3444, 1, 1140)\n"
     ]
    }
   ],
   "source": [
    "# even though they were converted to tensors before saving they need to be converted back becsuse when loading them with np it becomes an array again.\n",
    "X_train = np.load(\"../data/X_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "X_val   = np.load(\"../data/X_val.npy\")\n",
    "y_val   = np.load(\"../data/y_val.npy\")\n",
    "\n",
    "\n",
    "print(\"Before reshaping input to DataLoader:\")\n",
    "print(X_train.shape)\n",
    "print(\"Raw X_val shape:\",   X_val.shape)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), 1, -1)\n",
    "X_val   = X_val.reshape(len(X_val), 1, -1)\n",
    "\n",
    "print(\"after reshaping input to DataLoader:\")\n",
    "print(X_train.shape)\n",
    "print(\"Raw X_val shape:\",   X_val.shape)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b421ed00-fbb9-4374-a17a-2d95f25402a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataloaders \n",
    "\n",
    "train_dataset = EEGDataset(X_train, y_train)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                           \n",
    "val_dataset = EEGDataset(X_val, y_val)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51c33ac7-630b-4419-8395-d28aabd0a875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN BATCH SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL BATCH SHAPE: torch.Size([32, 1, 1140])\n",
      "X_train shape from file: torch.Size([13305, 1, 1140])\n",
      "X_val shape from file: torch.Size([3444, 1, 1140])\n",
      "First element type: <class 'torch.Tensor'>\n",
      "First element shape: torch.Size([1, 1140])\n"
     ]
    }
   ],
   "source": [
    "# Safety Check\n",
    "for xb, yb in train_loader:\n",
    "    print(\"TRAIN BATCH SHAPE:\", xb.shape)\n",
    "    break\n",
    "\n",
    "for xb, yb in val_loader:\n",
    "    print(\"VAL BATCH SHAPE:\", xb.shape)\n",
    "    break\n",
    "\n",
    "print(\"X_train shape from file:\", X_train.shape)\n",
    "print(\"X_val shape from file:\", X_val.shape)\n",
    "\n",
    "print(\"First element type:\", type(X_train[0]))\n",
    "print(\"First element shape:\", getattr(X_train[0], 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34bbd68f-ff07-4eaf-99be-f5399f1a8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialze the CNN\n",
    "\n",
    "n_channels = 1      # or however many EEG channels we have\n",
    "n_classes = 2        # ADHD vs Control\n",
    "\n",
    "model = ResNet1D(n_channels=n_channels, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a0a39a-01c4-4250-bb56-3dc86f6612ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letting the computer know what piece of hardware to run the training \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841ae2f-ec04-4a17-aa17-8167e424bb72",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bde9f48-121a-4de8-b6ce-b4e3e3d0c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy calculations\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = (preds == y).sum().item()\n",
    "    total = y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba2ce2b0-c52d-4157-8d8e-375695b232aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # put model in \"training mode\"\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        print(\"TRAIN LOOP SHAPE:\", X.shape)\n",
    "        break\n",
    "        \n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        logits = model(X)\n",
    "\n",
    "        # 2. Compute loss\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # 3. Zero out old gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training accuracy & loss\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "#only run to check if functional. and last i ran it was functional\n",
    "#train_one_epoch(model, train_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28d89e14-74fc-4123-8d07-9fbf4abd071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid loop with no gradient Updates \n",
    "# Very similar to the training loop, except this one sets the model to eval and its accompanied by other\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Do NOT track gradients\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            print(\"VAL LOOP SHAPE:\", X.shape)\n",
    "            break\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss = running_loss / total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "#only run to check if functional. and last i ran it was functional\n",
    "#validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "125ef7cb-8a43-45fb-a02f-3e43623581bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=20, lr=1e-3):\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\":  [],\n",
    "        \"val_loss\":   [],\n",
    "        \"val_acc\":    []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6f6f3-fa2b-4eb2-9cf6-48c56761a33d",
   "metadata": {},
   "source": [
    "## Training the architectiure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc8ad2d7-5e01-4200-8a54-97c181d610f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on cpu\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.7550 Acc: 0.4561 | Val Loss: 0.7821 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.7542 Acc: 0.4561 | Val Loss: 0.7933 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.7550 Acc: 0.4561 | Val Loss: 0.7441 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.7559 Acc: 0.4561 | Val Loss: 0.8077 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.7553 Acc: 0.4560 | Val Loss: 0.7968 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.7553 Acc: 0.4561 | Val Loss: 0.7433 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.7543 Acc: 0.4561 | Val Loss: 0.8072 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.7542 Acc: 0.4561 | Val Loss: 0.7800 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.7556 Acc: 0.4561 | Val Loss: 0.7866 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.7542 Acc: 0.4561 | Val Loss: 0.7533 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.7545 Acc: 0.4561 | Val Loss: 0.7732 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 12/20 | Train Loss: 0.7552 Acc: 0.4561 | Val Loss: 0.7733 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 13/20 | Train Loss: 0.7551 Acc: 0.4561 | Val Loss: 0.7984 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 14/20 | Train Loss: 0.7540 Acc: 0.4561 | Val Loss: 0.7522 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 15/20 | Train Loss: 0.7541 Acc: 0.4561 | Val Loss: 0.7742 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 16/20 | Train Loss: 0.7551 Acc: 0.4561 | Val Loss: 0.7832 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 17/20 | Train Loss: 0.7538 Acc: 0.4561 | Val Loss: 0.7768 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 18/20 | Train Loss: 0.7555 Acc: 0.4561 | Val Loss: 0.7910 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 19/20 | Train Loss: 0.7542 Acc: 0.4561 | Val Loss: 0.8020 Acc: 0.3885\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 20/20 | Train Loss: 0.7539 Acc: 0.4561 | Val Loss: 0.8002 Acc: 0.3885\n",
      "Model saved to ../models/resnet_eeg.pth\n"
     ]
    }
   ],
   "source": [
    "#Training the resnet architeciure on Raw data \n",
    "\n",
    "# Clear GPU cache before creating model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory cleared. Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Try to create model on GPU, fallback to CPU if OOM\n",
    "try:\n",
    "    resnet_raw = ResNet1D(n_channels=X_train.shape[1], n_classes=2).to(device)\n",
    "    print(f\"Model created on {device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower() or \"cuda\" in str(e).lower():\n",
    "        print(f\"CUDA OOM error. Falling back to CPU...\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch.cuda.empty_cache()\n",
    "        resnet_raw = ResNet1D(n_channels=X_train.shape[1], n_classes=2).to(device)\n",
    "        print(f\"Model created on CPU\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "history_resnet_raw = train_model(\n",
    "    model=resnet_raw,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Saving for Saliency later\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "torch.save(resnet_raw.state_dict(), \"../models/resnet_eeg.pth\")\n",
    "print(\"Model saved to ../models/resnet_eeg.pth\")\n",
    "# Can be loaded with the following code \n",
    "# model.load_state_dict(torch.load(\"resnet_eeg.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a826eeb-3d56-4f96-9316-d40a3b552279",
   "metadata": {},
   "source": [
    "## Compute saliency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93cd681b-62eb-441e-acc1-b271245ca707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency(model, X_batch, y_batch, device):\n",
    "    model.eval()\n",
    "    # Ensure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    X_batch = X_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    X_batch.requires_grad = True  # Enable gradient wrt input\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(X_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient wrt input\n",
    "    saliency = X_batch.grad.detach().abs()  # absolute gradient\n",
    "\n",
    "    return saliency.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d1121-6e74-4438-a419-6499e472b3ca",
   "metadata": {},
   "source": [
    "## Global saliency vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "514d45ff-2a2f-4957-b90b-46310c9822b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_saliencies = []\n",
    "\n",
    "for Xb, yb in val_loader:\n",
    "    sal = compute_saliency(model, Xb, yb, device)\n",
    "    all_saliencies.append(sal)\n",
    "\n",
    "all_saliencies = np.concatenate(all_saliencies, axis=0)   # (N_val, 1, 1140)\n",
    "\n",
    "# Average across samples and channel\n",
    "global_saliency = all_saliencies.mean(axis=0).squeeze(0)  # (1140,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "764b5fb6-161c-41d7-87d0-703b3c5d30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "global_saliency = global_saliency / global_saliency.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e4488fd-d338-4c98-9056-f4b8fa9328a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features per sample: 1140\n",
      "Kept:    625  (54.82%)\n",
      "Removed: 515  (45.18%)\n"
     ]
    }
   ],
   "source": [
    "# Measure how many features/time points are kept vs removed\n",
    "flat_mask = mask.reshape(-1)        # flatten in case it's not 1D\n",
    "num_total = flat_mask.size\n",
    "num_kept = flat_mask.sum()\n",
    "num_removed = num_total - num_kept\n",
    "percent_removed = (num_removed / num_total) * 100.0\n",
    "percent_kept = 100.0 - percent_removed\n",
    "\n",
    "print(f\"Total features per sample: {num_total}\")\n",
    "print(f\"Kept:    {num_kept:.0f}  ({percent_kept:.2f}%)\")\n",
    "print(f\"Removed: {num_removed:.0f}  ({percent_removed:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a499c-436a-447d-b557-dfe9bb32fcef",
   "metadata": {},
   "source": [
    "## Handling the Masked data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a4d6479-638d-420d-af92-c790d48eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying saliency mask to the dataset, actually just uses simple multiplication\n",
    "\n",
    "# Convert tensors back to numpy so we can multiply with mask\n",
    "X_train_np = X_train.cpu().numpy().reshape(len(X_train), 1140)\n",
    "X_val_np   = X_val.cpu().numpy().reshape(len(X_val), 1140)\n",
    "\n",
    "#applying the mask\n",
    "X_train_masked = X_train_np * mask\n",
    "X_val_masked   = X_val_np * mask\n",
    "\n",
    "#reshaping back to conv1d format \n",
    "X_train_masked = X_train_masked.reshape(len(X_train_masked), 1, 1140)\n",
    "X_val_masked   = X_val_masked.reshape(len(X_val_masked),   1, 1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ac2b8f1-9aa1-4ffe-bdf7-6f26e0165392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data loaders for the masked data  \n",
    "y_train_masked = y_train.clone()\n",
    "y_val_masked   = y_val.clone()\n",
    "\n",
    "X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
    "X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n",
    "\n",
    "#creating the masked dataset\n",
    "train_dataset_masked = EEGDataset(X_train_masked, y_train_masked)\n",
    "val_dataset_masked   = EEGDataset(X_val_masked,   y_val_masked)\n",
    "\n",
    "#dataLoaders\n",
    "train_loader_masked = DataLoader(train_dataset_masked, batch_size=32, shuffle=True)\n",
    "val_loader_masked   = DataLoader(val_dataset_masked,   batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c458a18e-a926-4267-8f64-e95f767ffc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1_/635jq8l52k10bb3t2l5v0_400000gn/T/ipykernel_16031/922651201.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
      "/var/folders/1_/635jq8l52k10bb3t2l5v0_400000gn/T/ipykernel_16031/922651201.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# converting masked data to tensors \n",
    "X_train_masked = torch.tensor(X_train_masked, dtype=torch.float32)\n",
    "y_train_masked = y_train.clone()      # same labels\n",
    "X_val_masked   = torch.tensor(X_val_masked,   dtype=torch.float32)\n",
    "y_val_masked   = y_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0737075c-c40b-45ad-a1f3-623bf5f8e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatging new dataset and dataloaders for the data \n",
    "train_dataset_masked = EEGDataset(X_train_masked, y_train_masked)\n",
    "val_dataset_masked   = EEGDataset(X_val_masked,   y_val_masked)\n",
    "\n",
    "train_loader_masked = DataLoader(train_dataset_masked, batch_size=32, shuffle=True)\n",
    "val_loader_masked   = DataLoader(val_dataset_masked,   batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e665442d-fbc3-45d5-a729-29f2af022e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASKED TRAIN BATCH: torch.Size([32, 1, 1140])\n",
      "MASKED VAL BATCH: torch.Size([32, 1, 1140])\n"
     ]
    }
   ],
   "source": [
    "# Checking shapes for troubleshooting purposes \n",
    "for xb, yb in train_loader_masked:\n",
    "    print(\"MASKED TRAIN BATCH:\", xb.shape)\n",
    "    break\n",
    "\n",
    "for xb, yb in val_loader_masked:\n",
    "    print(\"MASKED VAL BATCH:\", xb.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eec001-e79a-4d14-ace3-0712b7d4685b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1Dimentional implemeentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "516c4d5d-65d2-489c-9927-4480b69d43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eeg net 1Dimentional implemeentation \n",
    "\n",
    "class EEGNet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int = 2,\n",
    "        Chans: int = 1,        # number of input channels\n",
    "        Samples: int = 1140,   # length of the time series\n",
    "        F1: int = 8,\n",
    "        D: int = 2,\n",
    "        kernel_length: int = 64,\n",
    "        dropout: float = 0.25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.Chans = Chans\n",
    "        self.Samples = Samples\n",
    "\n",
    "        # 1) Temporal convolution\n",
    "        self.conv_temporal = nn.Conv1d(\n",
    "            in_channels=Chans,\n",
    "            out_channels=F1,\n",
    "            kernel_size=kernel_length,\n",
    "            padding=kernel_length // 2,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(F1)\n",
    "\n",
    "        # 2) Depthwise convolution\n",
    "        #    Each filter operates on its own channel (groups=F1), multiplied by D\n",
    "        self.conv_depthwise = nn.Conv1d(\n",
    "            in_channels=F1,\n",
    "            out_channels=F1 * D,\n",
    "            kernel_size=kernel_length,\n",
    "            padding=kernel_length // 2,\n",
    "            groups=F1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(F1 * D)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=4)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 3) Separable convolution\n",
    "        #    depthwise (groups=F1*D) + pointwise (1x1 conv)\n",
    "        self.conv_separable_depth = nn.Conv1d(\n",
    "            in_channels=F1 * D,\n",
    "            out_channels=F1 * D,\n",
    "            kernel_size=16,\n",
    "            padding=16 // 2,\n",
    "            groups=F1 * D,\n",
    "            bias=False\n",
    "        )\n",
    "        self.conv_separable_point = nn.Conv1d(\n",
    "            in_channels=F1 * D,\n",
    "            out_channels=F1 * D * 2,  # F2 = 2 * F1 * D\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(F1 * D * 2)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=8)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 4) Global average pooling + classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(F1 * D * 2, n_classes)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1, 1140)\n",
    "        \"\"\"\n",
    "        # 1) Temporal conv\n",
    "        x = self.conv_temporal(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.elu(x)\n",
    "\n",
    "        # 2) Depthwise conv\n",
    "        x = self.conv_depthwise(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 3) Separable conv\n",
    "        x = self.conv_separable_depth(x)\n",
    "        x = self.conv_separable_point(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 4) Global pooling + classifier\n",
    "        x = self.global_pool(x)     # (batch, channels, 1)\n",
    "        x = x.squeeze(-1)           # (batch, channels)\n",
    "        logits = self.classifier(x) # (batch, n_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b390c26-ae56-42f2-a9a9-7271df765a6c",
   "metadata": {},
   "source": [
    "## Training both datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55dce9b1-3324-4789-b9b9-1f24614eb87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.7013 Acc: 0.5439 | Val Loss: 0.6882 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.7014 Acc: 0.5439 | Val Loss: 0.6828 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.7011 Acc: 0.5440 | Val Loss: 0.6858 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.7030 Acc: 0.5439 | Val Loss: 0.6853 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.7019 Acc: 0.5439 | Val Loss: 0.6862 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.7025 Acc: 0.5441 | Val Loss: 0.6901 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.7020 Acc: 0.5439 | Val Loss: 0.6846 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.7019 Acc: 0.5439 | Val Loss: 0.6865 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.7025 Acc: 0.5439 | Val Loss: 0.6843 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.7024 Acc: 0.5439 | Val Loss: 0.6838 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.7024 Acc: 0.5439 | Val Loss: 0.6853 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 12/20 | Train Loss: 0.7020 Acc: 0.5440 | Val Loss: 0.6885 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 13/20 | Train Loss: 0.7019 Acc: 0.5439 | Val Loss: 0.6899 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 14/20 | Train Loss: 0.7022 Acc: 0.5438 | Val Loss: 0.6918 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 15/20 | Train Loss: 0.7022 Acc: 0.5439 | Val Loss: 0.6881 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 16/20 | Train Loss: 0.7025 Acc: 0.5439 | Val Loss: 0.6869 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 17/20 | Train Loss: 0.7026 Acc: 0.5440 | Val Loss: 0.6861 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 18/20 | Train Loss: 0.7019 Acc: 0.5440 | Val Loss: 0.6897 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 19/20 | Train Loss: 0.7021 Acc: 0.5439 | Val Loss: 0.6842 Acc: 0.6115\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 20/20 | Train Loss: 0.7023 Acc: 0.5441 | Val Loss: 0.6877 Acc: 0.6115\n",
      "EEGNet (raw) training time: 1249.67 seconds\n"
     ]
    }
   ],
   "source": [
    "# BEFORE filtering: EEGNet on raw EEG\n",
    "eegnet_raw = EEGNet1D(n_classes=2, Chans=1, Samples=1140).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "history_raw = train_model(\n",
    "    model=eegnet_raw,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=20,   # or 3 etc., whatever you used\n",
    "    lr=1e-3\n",
    ")\n",
    "eegnet_raw_train_time = time.time() - start_time\n",
    "print(f\"EEGNet (raw) training time: {eegnet_raw_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65f950cc-217a-403c-85b7-b31b2f8f953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 1/20 | Train Loss: 0.6960 Acc: 0.4732 | Val Loss: 0.7013 Acc: 0.4120\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 2/20 | Train Loss: 0.6963 Acc: 0.4744 | Val Loss: 0.7023 Acc: 0.4164\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 3/20 | Train Loss: 0.6960 Acc: 0.4702 | Val Loss: 0.7020 Acc: 0.4126\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 4/20 | Train Loss: 0.6959 Acc: 0.4741 | Val Loss: 0.7012 Acc: 0.4117\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 5/20 | Train Loss: 0.6959 Acc: 0.4712 | Val Loss: 0.7017 Acc: 0.4184\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 6/20 | Train Loss: 0.6956 Acc: 0.4767 | Val Loss: 0.7006 Acc: 0.4262\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 7/20 | Train Loss: 0.6960 Acc: 0.4731 | Val Loss: 0.7032 Acc: 0.4117\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 8/20 | Train Loss: 0.6960 Acc: 0.4729 | Val Loss: 0.6991 Acc: 0.4248\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 9/20 | Train Loss: 0.6961 Acc: 0.4715 | Val Loss: 0.7013 Acc: 0.4123\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 10/20 | Train Loss: 0.6959 Acc: 0.4755 | Val Loss: 0.7032 Acc: 0.4042\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 11/20 | Train Loss: 0.6960 Acc: 0.4771 | Val Loss: 0.7015 Acc: 0.4048\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 12/20 | Train Loss: 0.6961 Acc: 0.4749 | Val Loss: 0.7017 Acc: 0.4123\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 13/20 | Train Loss: 0.6959 Acc: 0.4735 | Val Loss: 0.7017 Acc: 0.4045\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 14/20 | Train Loss: 0.6961 Acc: 0.4774 | Val Loss: 0.7009 Acc: 0.4126\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 15/20 | Train Loss: 0.6959 Acc: 0.4740 | Val Loss: 0.7001 Acc: 0.3894\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 16/20 | Train Loss: 0.6957 Acc: 0.4691 | Val Loss: 0.6995 Acc: 0.4434\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 17/20 | Train Loss: 0.6960 Acc: 0.4700 | Val Loss: 0.7018 Acc: 0.4117\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 18/20 | Train Loss: 0.6958 Acc: 0.4761 | Val Loss: 0.7017 Acc: 0.4126\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 19/20 | Train Loss: 0.6958 Acc: 0.4778 | Val Loss: 0.6982 Acc: 0.3667\n",
      "TRAIN LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "VAL LOOP SHAPE: torch.Size([32, 1, 1140])\n",
      "Epoch 20/20 | Train Loss: 0.6955 Acc: 0.4783 | Val Loss: 0.7036 Acc: 0.4088\n",
      "EEGNet (masked) training time: 1251.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# AFTER filtering: EEGNet on masked EEG\n",
    "eegnet_masked = EEGNet1D(n_classes=2, Chans=1, Samples=1140).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "history_masked = train_model(\n",
    "    model=eegnet_masked,\n",
    "    train_loader=train_loader_masked,\n",
    "    val_loader=val_loader_masked,\n",
    "    device=device,\n",
    "    epochs=20,   # same as above\n",
    "    lr=1e-3\n",
    ")\n",
    "eegnet_masked_train_time = time.time() - start_time\n",
    "print(f\"EEGNet (masked) training time: {eegnet_masked_train_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3db5c7-eb60-41bf-b12e-79d57eed2e9d",
   "metadata": {},
   "source": [
    "## Simple evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62917346-36b6-4d19-a055-0a81b1514b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1D(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(7,), stride=(2,), padding=(3,))\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock1D(\n",
       "      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,))\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): ResidualBlock1D(\n",
       "      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_loaded = ResNet1D(n_channels=1, n_classes=2).to(device)\n",
    "resnet_loaded.load_state_dict(torch.load(\"../models/resnet_eeg.pth\", map_location=device))\n",
    "resnet_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7ec9200-2cee-49d3-994c-e16310e74b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with metrics \n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:      \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # ADHD vs Control: binary classification.\n",
    "    # If it ever becomes multi-class, change average=\"macro\".\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"binary\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75afc4be-d147-4cc4-b6c2-b218ed920c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/eeg-ml/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all three models and record inference time\n",
    "\n",
    "start = time.time()\n",
    "metrics_resnet = evaluate_model(resnet_loaded, val_loader, device)\n",
    "resnet_infer_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "metrics_eegnet_raw = evaluate_model(eegnet_raw, val_loader, device)\n",
    "eegnet_raw_infer_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "metrics_eegnet_masked = evaluate_model(eegnet_masked, val_loader_masked, device)\n",
    "eegnet_masked_infer_time = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3974493b-cacf-40a4-8f58-409306884f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet (raw):\n",
      "Accuracy:  0.3885\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "Inference time: 58.3219 seconds\n",
      "\n",
      "EEGNet (raw):\n",
      "Accuracy:  0.6353\n",
      "Precision: 0.6298\n",
      "Recall:    0.9791\n",
      "F1 score:  0.7665\n",
      "Inference time: 10.3454 seconds\n",
      "\n",
      "EEGNet (masked):\n",
      "Accuracy:  0.4271\n",
      "Precision: 0.6291\n",
      "Recall:    0.1538\n",
      "F1 score:  0.2472\n",
      "Inference time: 10.3429 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(name, metrics, infer_time):\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"Inference time: {infer_time:.4f} seconds\")\n",
    "    print()\n",
    "\n",
    "print_metrics(\"ResNet (raw)\", metrics_resnet, resnet_infer_time)\n",
    "print_metrics(\"EEGNet (raw)\", metrics_eegnet_raw, eegnet_raw_infer_time)\n",
    "print_metrics(\"EEGNet (masked)\", metrics_eegnet_masked, eegnet_masked_infer_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb4c77",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba942b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488d84e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e87663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcee47",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769963b7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0867775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec10124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a548f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a89ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
